{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giulia-cloud95/ERSU/blob/main/Chatbot_ERSU_(1)_ipynb_GENERALI_ITALIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kNcj_1lvFkC"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pypdf2 langchain faiss-cpu openai tiktoken\n",
        "!pip install -U langchain-community\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Personalizzazioni CSS\n",
        "st.set_page_config(page_title= \"INFO GENERALI ITALIA\",\n",
        "                   page_icon=\":credit_card:\"\n",
        "                  )\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .stApp {\n",
        "        background-color: \t#f5fffa;\n",
        "        color: #000000;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True)\n",
        "\n",
        "chiave = st.secrets[\"superkey\"]\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "\n",
        "st.header(\":credit_card: INFO GENERALI BOT :credit_card:\")\n",
        "\n",
        "from PIL import Image\n",
        "logo = Image.open(\"/content/winged-lion-majestic-lion-with-wings-illustration_691560-4889.jpeg.jpeg\")\n",
        "st.image(logo, width=800)\n",
        "# st.image(logo, use_column_width=True)\n",
        "\n",
        "# with st.sidebar:\n",
        "#  st.title(\"Carica i tuoi documenti\")\n",
        "#  file = st.file_uploader(\"Carica il tuo file\", type=\"pdf\")\n",
        "file = \"INPS (Generali).pdf\"\n",
        "file = \"DIARIA DA INTERRUZIONE DI ATTIVITA'.pdf\"\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "if file is not None:\n",
        "    testo_letto = PdfReader(file)\n",
        "\n",
        "    testo = \"\"\n",
        "    for pagina in testo_letto.pages:\n",
        "        testo = testo + pagina.extract_text()\n",
        "        # st.write(testo)\n",
        "\n",
        "    # Usiamo il text splitter di Langchain\n",
        "    testo_spezzato = RecursiveCharacterTextSplitter(\n",
        "        separators=\"\\n\",\n",
        "        chunk_size=1000, # Numero di caratteri per chunk\n",
        "        chunk_overlap=150,\n",
        "        length_function=len\n",
        "        )\n",
        "\n",
        "    pezzi = testo_spezzato.split_text(testo)\n",
        "    # st.write(pezzi)\n",
        "\n",
        "    # Generazione embeddings\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=chiave)\n",
        "\n",
        "    # Vector store - FAISS (by Facebook)\n",
        "    vector_store = FAISS.from_texts(pezzi, embeddings)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Gestione prompt\n",
        "# --------------------------------------------------\n",
        "\n",
        "    def invia():\n",
        "      st.session_state.domanda_inviata = st.session_state.domanda\n",
        "      # salva il contenuto di input in domanda_inviata\n",
        "      st.session_state.domanda = \"\"\n",
        "      # reset dopo invio\n",
        "\n",
        "    st.text_input(\"Chiedi al chatbot:\", key=\"domanda\", on_change=invia)\n",
        "    # key=\"domanda\": assegna a st.session_state ciò che scriviamo (domanda)\n",
        "    # Ogni volta che l’utente modifica il campo e preme Invio,\n",
        "    # la funzione invia() viene chiamata.\n",
        "\n",
        "    domanda = st.session_state.get(\"domanda_inviata\", \"\")\n",
        "    # Recupera il valore salvato in \"domanda_inviata\".\n",
        "    # Se \"domanda_inviata\" non è ancora stato definito (es. al primo avvio dell'app),\n",
        "    # allora il valore predefinito sarà \"\" (secondo argomento dell'istruzione)\n",
        "\n",
        "# --------------------------------------------------\n",
        "\n",
        "    if domanda:\n",
        "      # st.write(\"Sto cercando le informazioni che mi hai richiesto...\")\n",
        "      rilevanti = vector_store.similarity_search(domanda)\n",
        "\n",
        "      # Definiamo l'LLM\n",
        "      llm = ChatOpenAI(\n",
        "          openai_api_key = chiave,\n",
        "          temperature = 1.0,\n",
        "          max_tokens = 1000,\n",
        "          model_name = \"gpt-3.5-turbo-0125\")\n",
        "      # https://platform.openai.com/docs/models/compare\n",
        "\n",
        "      # Output\n",
        "      # Chain: prendi la domanda, individua i frammenti rilevanti,\n",
        "      # passali all'LLM, genera la risposta\n",
        "      chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "      risposta = chain.run(input_documents = rilevanti, question = domanda)\n",
        "      st.write(risposta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26a7f02-f43a-44c4-f86b-59dd22a4b695",
        "id": "lPdMXP5ElfyB"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aWEg7KnyUf3",
        "outputId": "1d47867a-2007-4f03-db0d-43d39ca23d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.198.197.222\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://loud-parrots-wave.loca.lt\n"
          ]
        }
      ]
    }
  ]
}